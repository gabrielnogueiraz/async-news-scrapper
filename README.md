# ğŸš€ Async News Scraper

Sistema assÃ­ncrono de alto desempenho para coleta e exposiÃ§Ã£o de notÃ­cias do portal G1, construÃ­do com arquitetura profissional e foco em escalabilidade.

## ğŸ“‹ DescriÃ§Ã£o

O **Async News Scraper** Ã© uma aplicaÃ§Ã£o completa que realiza scraping de manchetes do G1 de forma assÃ­ncrona, armazena os dados em banco SQLite e expÃµe endpoints REST para consulta e execuÃ§Ã£o de novas coletas. O projeto foi desenvolvido seguindo as melhores prÃ¡ticas de engenharia de software, com cÃ³digo limpo, tipagem estÃ¡tica completa e performance otimizada.

## âš¡ Performance em ProduÃ§Ã£o

A API foi submetida a rigorosos testes de carga para validar sua capacidade de lidar com trÃ¡fego real em produÃ§Ã£o:

### ğŸ“Š Resultados de Load Testing

**ConfiguraÃ§Ã£o do Teste:**
- **25 usuÃ¡rios concorrentes** realizando requisiÃ§Ãµes simultÃ¢neas
- **250 requisiÃ§Ãµes totais** distribuÃ­das entre mÃºltiplos endpoints
- **100% de taxa de sucesso** - zero falhas sob carga

**MÃ©tricas de Performance:**

| MÃ©trica | Resultado | AvaliaÃ§Ã£o |
|---------|-----------|-----------|
| **Throughput** | 33.52 req/s | Alta capacidade de processamento |
| **LatÃªncia Mediana (P50)** | 33.39 ms | Resposta extremamente rÃ¡pida |
| **LatÃªncia P95** | 2.80 segundos | 95% das requisiÃ§Ãµes abaixo de 3s |
| **Taxa de Sucesso** | 100% | Zero erros sob concorrÃªncia |
| **Uso de MemÃ³ria** | 66.44 MB | Footprint otimizado |

**Performance por Endpoint:**

| Endpoint | Tempo MÃ©dio | RequisiÃ§Ãµes |
|----------|-------------|-------------|
| `GET /` | 27.84 ms | 75 |
| `GET /news` | 47.68 ms | 75 |
| `GET /health` | 718.62 ms | 100 |

**Destaques:**
- âœ… **Escalabilidade Comprovada**: Suporta 25+ usuÃ¡rios simultÃ¢neos sem degradaÃ§Ã£o
- âœ… **Baixa LatÃªncia**: 50% das requisiÃ§Ãµes respondem em menos de 34ms
- âœ… **Alta Confiabilidade**: 100% de uptime durante testes de stress
- âœ… **EficiÃªncia de Recursos**: Consumo de memÃ³ria otimizado para ambientes cloud

## ğŸ› ï¸ Stack TecnolÃ³gica

- **Python 3.11+** - Linguagem base com recursos modernos
- **FastAPI** - Framework web assÃ­ncrono de alta performance
- **SQLAlchemy 2.0** - ORM com suporte async/await
- **aiosqlite** - Driver SQLite assÃ­ncrono
- **aiohttp** - Cliente HTTP assÃ­ncrono para scraping
- **BeautifulSoup4** - Parser HTML para extraÃ§Ã£o de dados
- **Pydantic** - ValidaÃ§Ã£o de dados e serializaÃ§Ã£o
- **Uvicorn** - Servidor ASGI de produÃ§Ã£o

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚  â† Camada de API (endpoints REST)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper    â”‚  â† Camada de serviÃ§o (lÃ³gica de negÃ³cio)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ SQLAlchemy  â”‚  â† Camada de persistÃªncia (ORM async)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   SQLite    â”‚  â† Banco de dados
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **Scraping**: `aiohttp` faz requisiÃ§Ã£o assÃ­ncrona ao G1
2. **Parsing**: `BeautifulSoup` extrai manchetes e links
3. **PersistÃªncia**: `SQLAlchemy` salva dados no SQLite (async)
4. **API**: `FastAPI` expÃµe endpoints para consulta e trigger de scraping

## ğŸ“ Estrutura do Projeto

```
async-news-scrapper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py              # Endpoints FastAPI
â”‚   â”œâ”€â”€ db.py               # ConfiguraÃ§Ã£o do banco async
â”‚   â”œâ”€â”€ main.py             # Entry point da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ models.py           # Modelos SQLAlchemy
â”‚   â”œâ”€â”€ schemas.py          # Schemas Pydantic
â”‚   â””â”€â”€ scrapper/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ news_scrapper.py # LÃ³gica de scraping
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py         # ConfiguraÃ§Ã£o de fixtures
â”‚   â”œâ”€â”€ test_api.py         # Testes dos endpoints
â”‚   â”œâ”€â”€ test_scraper.py     # Testes do scraper
â”‚   â””â”€â”€ test_models.py      # Testes dos models
â”œâ”€â”€ benchmarks/             # Sistema de benchmarking
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py          # Coleta de mÃ©tricas
â”‚   â”œâ”€â”€ scraper_instrumented.py # Scraper instrumentado
â”‚   â”œâ”€â”€ reporter.py         # GeraÃ§Ã£o de relatÃ³rios
â”‚   â”œâ”€â”€ compare.py          # ComparaÃ§Ã£o de benchmarks
â”‚   â””â”€â”€ README.md           # DocumentaÃ§Ã£o dos benchmarks
â”œâ”€â”€ run_benchmark.py        # Script principal de benchmark
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ pytest.ini             # ConfiguraÃ§Ã£o do pytest
â”œâ”€â”€ Dockerfile             # Container Docker
â”œâ”€â”€ .env.example           # VariÃ¡veis de ambiente
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Como Executar

### PrÃ©-requisitos

- Python 3.11 ou superior
- pip

### InstalaÃ§Ã£o Local

1. **Clone o repositÃ³rio**

```bash
git clone <repository-url>
cd async-news-scrapper
```

2. **Crie um ambiente virtual**

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

3. **Instale as dependÃªncias**

```bash
pip install -r requirements.txt
```

> âš ï¸ **Problemas na instalaÃ§Ã£o?** Consulte o [TROUBLESHOOTING.md](TROUBLESHOOTING.md) para soluÃ§Ãµes de erros comuns (Rust/Cargo, ModuleNotFoundError, etc.)

4. **Execute a aplicaÃ§Ã£o**

```bash
python -m src.main
```

Ou diretamente com uvicorn:

```bash
uvicorn src.api:app --reload --host 0.0.0.0 --port 8000
```

A API estarÃ¡ disponÃ­vel em: `http://localhost:8000`

### Executar Testes

Execute a suite completa de testes:

```bash
pytest
```

Com cobertura de cÃ³digo:

```bash
pytest --cov=src --cov-report=html
```

Executar testes especÃ­ficos:

```bash
# Testar apenas a API
pytest tests/test_api.py

# Testar apenas o scraper
pytest tests/test_scraper.py

# Testar apenas os models
pytest tests/test_models.py
```

### ExecuÃ§Ã£o com Docker

1. **Build da imagem**

```bash
docker build -t async-news-scraper .
```

2. **Execute o container**

```bash
docker run -d -p 8000:8000 --name news-scraper async-news-scraper
```

3. **Acesse a aplicaÃ§Ã£o**

```
http://localhost:8000
```

## ğŸ“¡ Endpoints da API

### `GET /`

InformaÃ§Ãµes bÃ¡sicas do serviÃ§o

**Response:**

```json
{
  "service": "Async News Scraper",
  "status": "running",
  "endpoints": ["/news", "/scrape"]
}
```

### `GET /news`

Retorna todas as notÃ­cias armazenadas, ordenadas por data (mais recentes primeiro)

**Query Parameters:**

- `limit` (int, default: 100) - NÃºmero mÃ¡ximo de resultados
- `offset` (int, default: 0) - Offset para paginaÃ§Ã£o

**Response:**

```json
[
  {
    "id": 1,
    "title": "TÃ­tulo da notÃ­cia",
    "url": "https://g1.globo.com/...",
    "created_at": "2024-01-15T10:30:00"
  }
]
```

### `POST /scrape`

Executa uma nova coleta de notÃ­cias do G1

**Response:**

```json
{
  "success": true,
  "news_added": 15,
  "message": "Successfully scraped and added 15 new articles"
}
```

### `GET /health`

Health check do serviÃ§o

**Response:**

```json
{
  "status": "healthy",
  "service": "async-news-scraper"
}
```

## ğŸ§ª Exemplos de Uso

### cURL

**Listar notÃ­cias:**

```bash
curl http://localhost:8000/news
```

**Executar scraping:**

```bash
curl -X POST http://localhost:8000/scrape
```

### Python

```python
import httpx
import asyncio

async def main():
    async with httpx.AsyncClient() as client:
        # Executar scraping
        response = await client.post("http://localhost:8000/scrape")
        print(response.json())

        # Buscar notÃ­cias
        response = await client.get("http://localhost:8000/news?limit=10")
        print(response.json())

asyncio.run(main())
```

### JavaScript/TypeScript

```typescript
// Executar scraping
const scrapeResponse = await fetch("http://localhost:8000/scrape", {
  method: "POST",
});
const scrapeData = await scrapeResponse.json();

// Buscar notÃ­cias
const newsResponse = await fetch("http://localhost:8000/news?limit=10");
const newsData = await newsResponse.json();
```

## ğŸ”§ ConfiguraÃ§Ã£o

Copie `.env.example` para `.env` e ajuste as variÃ¡veis conforme necessÃ¡rio:

```env
DATABASE_URL=sqlite+aiosqlite:///./news.db
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=info
```

## ğŸ¯ CaracterÃ­sticas TÃ©cnicas

### Performance

- **100% AssÃ­ncrono**: Toda a stack utiliza async/await
- **Scraping Concorrente**: MÃºltiplas requisiÃ§Ãµes paralelas
- **Connection Pooling**: Gerenciamento eficiente de conexÃµes
- **Retry Logic**: ResiliÃªncia a falhas de rede com backoff exponencial

### Qualidade de CÃ³digo

- **Type Hints**: Tipagem estÃ¡tica completa
- **Clean Code**: CÃ³digo autoexplicativo sem comentÃ¡rios desnecessÃ¡rios
- **Separation of Concerns**: Camadas bem definidas (API, Service, Data)
- **Error Handling**: Tratamento robusto de exceÃ§Ãµes
- **Test Coverage**: Suite completa de testes unitÃ¡rios e de integraÃ§Ã£o

### SeguranÃ§a

- **SQL Injection Protection**: ORM previne injeÃ§Ãµes
- **Input Validation**: Pydantic valida todas as entradas
- **Timeout Management**: ProteÃ§Ã£o contra requisiÃ§Ãµes travadas
- **Unique Constraints**: Previne duplicaÃ§Ã£o de notÃ­cias

## ğŸ“Š Modelo de Dados

```python
class News:
    id: int                 # Primary key auto-increment
    title: str              # TÃ­tulo da notÃ­cia (max 500 chars)
    url: str                # URL Ãºnica da notÃ­cia (max 1000 chars)
    created_at: datetime    # Timestamp de criaÃ§Ã£o
```

## ğŸ” DocumentaÃ§Ã£o Interativa

Acesse a documentaÃ§Ã£o automÃ¡tica da API:

- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

## ğŸ§ª Testes

O projeto inclui uma suite completa de testes cobrindo:

### Testes de API (`test_api.py`)

- âœ… Health checks e endpoints bÃ¡sicos
- âœ… Listagem de notÃ­cias com paginaÃ§Ã£o
- âœ… OrdenaÃ§Ã£o por data
- âœ… ExecuÃ§Ã£o de scraping
- âœ… ValidaÃ§Ã£o de schemas
- âœ… Tratamento de erros

### Testes de Scraper (`test_scraper.py`)

- âœ… InicializaÃ§Ã£o do scraper
- âœ… Fetch de pÃ¡ginas com retry
- âœ… Parsing de HTML
- âœ… Salvamento de notÃ­cias
- âœ… PrevenÃ§Ã£o de duplicatas
- âœ… Scraping completo end-to-end

### Testes de Models (`test_models.py`)

- âœ… CriaÃ§Ã£o de registros
- âœ… Constraints de unicidade
- âœ… Timestamps automÃ¡ticos
- âœ… Queries e filtros

**Executar todos os testes:**

```bash
pytest -v
```

**Com relatÃ³rio de cobertura:**

```bash
pytest --cov=src --cov-report=term-missing
```

## ğŸ“Š Benchmarks de Performance

O projeto inclui um sistema profissional de benchmarking para medir o desempenho do scraper:

### MÃ©tricas Coletadas

- â±ï¸ **Tempo de ExecuÃ§Ã£o**: DuraÃ§Ã£o total e latÃªncia por requisiÃ§Ã£o
- ğŸš€ **Throughput**: RequisiÃ§Ãµes por segundo
- ğŸ’¾ **MemÃ³ria**: Uso de RAM (RSS + tracemalloc)
- âš¡ **CPU**: Uso percentual e tempo de CPU
- ğŸŒ **Rede**: Volume de dados enviados/recebidos
- âœ… **Taxa de Sucesso**: Confiabilidade das requisiÃ§Ãµes

### Executar Benchmarks

**Benchmark simples:**

```bash
python run_benchmark.py
```

**Benchmark com mÃºltiplas iteraÃ§Ãµes (mais preciso):**

```bash
python run_benchmark.py 3
```

**Usando Makefile:**

```bash
make benchmark          # ExecuÃ§Ã£o Ãºnica
make benchmark-multi    # 3 iteraÃ§Ãµes
make benchmark-compare  # Comparar resultados histÃ³ricos
```

### RelatÃ³rios Gerados

O benchmark gera trÃªs tipos de relatÃ³rios em `benchmarks/results/`:

1. **Console**: Output formatado no terminal
2. **JSON**: `benchmark_results.json` - Para anÃ¡lise programÃ¡tica
3. **Markdown**: `benchmark_results.md` - Para documentaÃ§Ã£o

### Exemplo de Output

```
================================================================================
        ASYNC NEWS SCRAPER - PERFORMANCE BENCHMARK REPORT
================================================================================

ğŸ“Š EXECUTION SUMMARY
--------------------------------------------------------------------------------
Total Duration:           2.3456 seconds
News Scraped:             45
Throughput:               0.43 req/s

ğŸ’¾ MEMORY METRICS
--------------------------------------------------------------------------------
Memory Used (RSS):        15.23 MB
Peak Memory:              12.45 MB

âš¡ CPU METRICS
--------------------------------------------------------------------------------
Average CPU Usage:        8.45%
CPU Efficiency:           5.26%

ğŸ“ˆ PERFORMANCE ANALYSIS
--------------------------------------------------------------------------------
Bottleneck Analysis:      I/O Bound (Good for async operations)
```

Para mais detalhes, consulte [benchmarks/README.md](benchmarks/README.md)

## ğŸ”¥ Load Testing da API

O projeto inclui um sistema profissional de **teste de carga** para medir a performance da API sob concorrÃªncia:

### O que Ã© Load Testing?

Diferente do benchmark (que testa o scraper isolado), o **load test simula mÃºltiplos usuÃ¡rios simultÃ¢neos** acessando a API para validar:

- ğŸš€ **Capacidade de concorrÃªncia**: Quantos usuÃ¡rios simultÃ¢neos a API suporta
- â±ï¸ **Response time sob carga**: LatÃªncia real com mÃºltiplos usuÃ¡rios
- ğŸ’¾ **Uso de memÃ³ria**: Target < 77MB
- âœ… **Confiabilidade**: Taxa de sucesso sob stress

### Como Executar

**1. Inicie o servidor (Terminal 1):**

```bash
python -m src.main
```

**2. Execute o load test (Terminal 2):**

```bash
# Teste mÃ©dio (25 usuÃ¡rios, 250 requisiÃ§Ãµes)
python run_load_test.py

# Teste leve (10 usuÃ¡rios)
python run_load_test.py light

# Teste pesado (50 usuÃ¡rios)
python run_load_test.py heavy

# Stress test (100 usuÃ¡rios)
python run_load_test.py stress

# Todos os cenÃ¡rios
python run_load_test.py all
```

**Usando Makefile:**

```bash
make load-test          # MÃ©dio
make load-test-heavy    # Pesado
make load-test-stress   # Stress
make load-test-all      # Todos
```

### MÃ©tricas Coletadas

- **Response Time**: avg, min, max, p50, p95, p99
- **Throughput**: RequisiÃ§Ãµes por segundo
- **Success Rate**: Taxa de sucesso sob concorrÃªncia
- **Memory Usage**: Uso de memÃ³ria (target: < 77MB)
- **CPU Usage**: Uso de CPU sob carga
- **Status Codes**: DistribuiÃ§Ã£o de cÃ³digos HTTP

### Exemplo de Output

```
âš™ï¸  TEST CONFIGURATION
--------------------------------------------------------------------------------
Concurrent Users:         25
Total Requests:           250
Test Duration:            5.23 seconds

ğŸ“Š REQUEST SUMMARY
--------------------------------------------------------------------------------
Success Rate:             100.00%
Throughput:               47.76 req/s

â±ï¸  RESPONSE TIME STATISTICS
--------------------------------------------------------------------------------
Average Response Time:    45.23 ms
P95:                      89.45 ms
P99:                      134.56 ms

ğŸ’¾ MEMORY METRICS
--------------------------------------------------------------------------------
Memory End:               52.34 MB
âœ… Memory End < 77MB:     PASS
```

Para mais detalhes, consulte [LOAD_TEST_QUICKSTART.md](LOAD_TEST_QUICKSTART.md) e [load_tests/README.md](load_tests/README.md)

## ğŸ¤ Contribuindo

Este projeto segue padrÃµes profissionais de desenvolvimento:

1. CÃ³digo deve ser assÃ­ncrono
2. Type hints sÃ£o obrigatÃ³rios
3. Siga PEP 8
4. Mantenha a separaÃ§Ã£o de camadas
5. Escreva cÃ³digo autoexplicativo
6. Todos os PRs devem incluir testes

## ğŸ“ LicenÃ§a

MIT License
