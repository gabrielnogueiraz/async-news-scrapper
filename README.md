# ğŸš€ Async News Scraper

Sistema assÃ­ncrono de alto desempenho para coleta e exposiÃ§Ã£o de notÃ­cias do portal G1, construÃ­do com arquitetura profissional e foco em escalabilidade.

## ğŸ“‹ DescriÃ§Ã£o

O **Async News Scraper** Ã© uma aplicaÃ§Ã£o completa que realiza scraping de manchetes do G1 de forma assÃ­ncrona, armazena os dados em banco SQLite e expÃµe endpoints REST para consulta e execuÃ§Ã£o de novas coletas. O projeto foi desenvolvido seguindo as melhores prÃ¡ticas de engenharia de software, com cÃ³digo limpo, tipagem estÃ¡tica completa e performance otimizada.

## ğŸ› ï¸ Stack TecnolÃ³gica

- **Python 3.11+** - Linguagem base com recursos modernos
- **FastAPI** - Framework web assÃ­ncrono de alta performance
- **SQLAlchemy 2.0** - ORM com suporte async/await
- **aiosqlite** - Driver SQLite assÃ­ncrono
- **aiohttp** - Cliente HTTP assÃ­ncrono para scraping
- **BeautifulSoup4** - Parser HTML para extraÃ§Ã£o de dados
- **Pydantic** - ValidaÃ§Ã£o de dados e serializaÃ§Ã£o
- **Uvicorn** - Servidor ASGI de produÃ§Ã£o

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚  â† Camada de API (endpoints REST)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper    â”‚  â† Camada de serviÃ§o (lÃ³gica de negÃ³cio)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ SQLAlchemy  â”‚  â† Camada de persistÃªncia (ORM async)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   SQLite    â”‚  â† Banco de dados
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **Scraping**: `aiohttp` faz requisiÃ§Ã£o assÃ­ncrona ao G1
2. **Parsing**: `BeautifulSoup` extrai manchetes e links
3. **PersistÃªncia**: `SQLAlchemy` salva dados no SQLite (async)
4. **API**: `FastAPI` expÃµe endpoints para consulta e trigger de scraping

## ğŸ“ Estrutura do Projeto

```
async-news-scrapper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py              # Endpoints FastAPI
â”‚   â”œâ”€â”€ db.py               # ConfiguraÃ§Ã£o do banco async
â”‚   â”œâ”€â”€ main.py             # Entry point da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ models.py           # Modelos SQLAlchemy
â”‚   â”œâ”€â”€ schemas.py          # Schemas Pydantic
â”‚   â””â”€â”€ scrapper/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ news_scrapper.py # LÃ³gica de scraping
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ Dockerfile             # Container Docker
â”œâ”€â”€ .env.example           # VariÃ¡veis de ambiente
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Como Executar

### PrÃ©-requisitos

- Python 3.11 ou superior
- pip

### InstalaÃ§Ã£o Local

1. **Clone o repositÃ³rio**
```bash
git clone <repository-url>
cd async-news-scrapper
```

2. **Crie um ambiente virtual**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

3. **Instale as dependÃªncias**
```bash
pip install -r requirements.txt
```

4. **Execute a aplicaÃ§Ã£o**
```bash
python -m src.main
```

Ou diretamente com uvicorn:
```bash
uvicorn src.api:app --reload --host 0.0.0.0 --port 8000
```

A API estarÃ¡ disponÃ­vel em: `http://localhost:8000`

### ExecuÃ§Ã£o com Docker

1. **Build da imagem**
```bash
docker build -t async-news-scraper .
```

2. **Execute o container**
```bash
docker run -d -p 8000:8000 --name news-scraper async-news-scraper
```

3. **Acesse a aplicaÃ§Ã£o**
```
http://localhost:8000
```

## ğŸ“¡ Endpoints da API

### `GET /`
InformaÃ§Ãµes bÃ¡sicas do serviÃ§o

**Response:**
```json
{
  "service": "Async News Scraper",
  "status": "running",
  "endpoints": ["/news", "/scrape"]
}
```

### `GET /news`
Retorna todas as notÃ­cias armazenadas, ordenadas por data (mais recentes primeiro)

**Query Parameters:**
- `limit` (int, default: 100) - NÃºmero mÃ¡ximo de resultados
- `offset` (int, default: 0) - Offset para paginaÃ§Ã£o

**Response:**
```json
[
  {
    "id": 1,
    "title": "TÃ­tulo da notÃ­cia",
    "url": "https://g1.globo.com/...",
    "created_at": "2024-01-15T10:30:00"
  }
]
```

### `POST /scrape`
Executa uma nova coleta de notÃ­cias do G1

**Response:**
```json
{
  "success": true,
  "news_added": 15,
  "message": "Successfully scraped and added 15 new articles"
}
```

### `GET /health`
Health check do serviÃ§o

**Response:**
```json
{
  "status": "healthy",
  "service": "async-news-scraper"
}
```

## ğŸ§ª Exemplos de Uso

### cURL

**Listar notÃ­cias:**
```bash
curl http://localhost:8000/news
```

**Executar scraping:**
```bash
curl -X POST http://localhost:8000/scrape
```

### Python

```python
import httpx
import asyncio

async def main():
    async with httpx.AsyncClient() as client:
        # Executar scraping
        response = await client.post("http://localhost:8000/scrape")
        print(response.json())
        
        # Buscar notÃ­cias
        response = await client.get("http://localhost:8000/news?limit=10")
        print(response.json())

asyncio.run(main())
```

### JavaScript/TypeScript

```typescript
// Executar scraping
const scrapeResponse = await fetch('http://localhost:8000/scrape', {
  method: 'POST'
});
const scrapeData = await scrapeResponse.json();

// Buscar notÃ­cias
const newsResponse = await fetch('http://localhost:8000/news?limit=10');
const newsData = await newsResponse.json();
```

## ğŸ”§ ConfiguraÃ§Ã£o

Copie `.env.example` para `.env` e ajuste as variÃ¡veis conforme necessÃ¡rio:

```env
DATABASE_URL=sqlite+aiosqlite:///./news.db
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=info
```

## ğŸ¯ CaracterÃ­sticas TÃ©cnicas

### Performance
- **100% AssÃ­ncrono**: Toda a stack utiliza async/await
- **Scraping Concorrente**: MÃºltiplas requisiÃ§Ãµes paralelas
- **Connection Pooling**: Gerenciamento eficiente de conexÃµes
- **Retry Logic**: ResiliÃªncia a falhas de rede com backoff exponencial

### Qualidade de CÃ³digo
- **Type Hints**: Tipagem estÃ¡tica completa
- **Clean Code**: CÃ³digo autoexplicativo sem comentÃ¡rios desnecessÃ¡rios
- **Separation of Concerns**: Camadas bem definidas (API, Service, Data)
- **Error Handling**: Tratamento robusto de exceÃ§Ãµes

### SeguranÃ§a
- **SQL Injection Protection**: ORM previne injeÃ§Ãµes
- **Input Validation**: Pydantic valida todas as entradas
- **Timeout Management**: ProteÃ§Ã£o contra requisiÃ§Ãµes travadas
- **Unique Constraints**: Previne duplicaÃ§Ã£o de notÃ­cias

## ğŸ“Š Modelo de Dados

```python
class News:
    id: int                 # Primary key auto-increment
    title: str              # TÃ­tulo da notÃ­cia (max 500 chars)
    url: str                # URL Ãºnica da notÃ­cia (max 1000 chars)
    created_at: datetime    # Timestamp de criaÃ§Ã£o
```

## ğŸ” DocumentaÃ§Ã£o Interativa

Acesse a documentaÃ§Ã£o automÃ¡tica da API:

- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

## ğŸ¤ Contribuindo

Este projeto segue padrÃµes profissionais de desenvolvimento:

1. CÃ³digo deve ser assÃ­ncrono
2. Type hints sÃ£o obrigatÃ³rios
3. Siga PEP 8
4. Mantenha a separaÃ§Ã£o de camadas
5. Escreva cÃ³digo autoexplicativo

## ğŸ“ LicenÃ§a

MIT License

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido com foco em qualidade, performance e boas prÃ¡ticas de engenharia de software.
